{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and Implementation\n",
    "As an accompaniment to the videos we will follow the particle filter algorithm process and implementation details.\n",
    "### Particle Filter Algorithm Steps and Inputs\n",
    "The flowchart below represents the steps of the particle filter algorithm as well as its inputs.\n",
    "![](https://video.udacity-data.com/topher/2017/August/5989f54e_02-l-pseudocode.00-00-47-13.still006/02-l-pseudocode.00-00-47-13.still006.png)\n",
    "\n",
    "### Psuedo Code\n",
    "This is an outline of steps you will need to take with your code in order to implement a particle filter for localizing an autonomous vehicle. The pseudo code steps correspond to the steps in the algorithm flow chart, initialization, prediction, particle weight updates, and resampling. Python implementation of these steps was covered in the previous lesson.\n",
    "\n",
    "![](https://video.udacity-data.com/topher/2017/August/5989f6fb_02-l-pseudocode.00-00-14-28.still001/02-l-pseudocode.00-00-14-28.still001.png)\n",
    "\n",
    "At the initialization step we estimate our position from GPS input. The subsequent steps in the process will refine this estimate to localize our vehicle.\n",
    "\n",
    "![](https://video.udacity-data.com/topher/2017/August/5989f70c_02-l-pseudocode.00-00-16-01.still002/02-l-pseudocode.00-00-16-01.still002.png)\n",
    "\n",
    "During the prediction step we add the control input (yaw rate & velocity) for all particles\n",
    "\n",
    "![](https://video.udacity-data.com/topher/2017/August/5989f719_02-l-pseudocode.00-00-30-05.still003/02-l-pseudocode.00-00-30-05.still003.png)\n",
    "\n",
    "During the update step, we update our particle weights using map landmark positions and feature measurements.\n",
    "\n",
    "![](https://video.udacity-data.com/topher/2017/August/5989f726_02-l-pseudocode.00-00-35-08.still004/02-l-pseudocode.00-00-35-08.still004.png)\n",
    "\n",
    "During resampling we will resample M times (M is range of 0 to length_of_particleArray) drawing a particle i (i is the particle index) proportional to its weight . Sebastian covered one implementation of this in his discussion and implementation of a resampling wheel.\n",
    "\n",
    "![](https://video.udacity-data.com/topher/2017/August/5989f736_02-l-pseudocode.00-00-40-01.still005/02-l-pseudocode.00-00-40-01.still005.png)\n",
    "\n",
    "The new set of particles represents the Bayes filter posterior probability. We now have a refined estimate of the vehicles position based on input evidence.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "![](https://video.udacity-data.com/topher/2017/August/598a0660_03-l-initialization.00-01-53-01.still001/03-l-initialization.00-01-53-01.still001.png)\n",
    "\n",
    "The most practical way to initialize our particles and generate real time output, is to make an initial estimate using GPS input. As with all sensor based operations, this step is impacted by noise.\n",
    "\n",
    "#### Project Implementation\n",
    "- Particles shall be implemented by sampling a Gaussian distribution, taking into account Gaussian sensor noise around the initial GPS position and heading estimates.\n",
    "- Use the C++ standard library normal distribution and C++ standard library random engine functions to sample positions around GPS measurements.\n",
    "\n",
    "```cpp\n",
    "/**\n",
    " * print_samples_sol.cpp\n",
    " *\n",
    " * SOLUTION CODE\n",
    " * \n",
    " * Print out to the terminal 3 samples from a normal distribution with\n",
    " * mean equal to the GPS position and IMU heading measurements and\n",
    " * standard deviation of 2 m for the x and y position and 0.05 radians\n",
    " * for the heading of the car. \n",
    " *\n",
    " * Author: Tiffany Huang\n",
    " */\n",
    "\n",
    "#include <iostream>\n",
    "#include <random> // Need this for sampling from distributions\n",
    "\n",
    "using std::normal_distribution;\n",
    "\n",
    "/**\n",
    " * Prints samples of x, y and theta from a normal distribution\n",
    " * @param gps_x   GPS provided x position\n",
    " * @param gps_y   GPS provided y position\n",
    " * @param theta   GPS provided yaw\n",
    " */\n",
    "void printSamples(double gps_x, double gps_y, double theta);\n",
    "\n",
    "\n",
    "int main() {\n",
    "  \n",
    "  // Set GPS provided state of the car.\n",
    "  double gps_x = 4983;\n",
    "  double gps_y = 5029;\n",
    "  double theta = 1.201;\n",
    "  \n",
    "  // Sample from the GPS provided position.\n",
    "  printSamples(gps_x, gps_y, theta);\n",
    "  \n",
    "  return 0;\n",
    "}\n",
    "\n",
    "\n",
    "void printSamples(double gps_x, double gps_y, double theta) {\n",
    "  std::default_random_engine gen;\n",
    "  double std_x, std_y, std_theta;  // Standard deviations for x, y, and theta\n",
    "\n",
    "  // TODO: Set standard deviations for x, y, and theta\n",
    "  std_x = 2;\n",
    "  std_y = 2;\n",
    "  std_theta = 0.05; \n",
    "\n",
    "  // This line creates a normal (Gaussian) distribution for x\n",
    "  normal_distribution<double> dist_x(gps_x, std_x);\n",
    "  \n",
    "  // TODO: Create normal distributions for y and theta\n",
    "  normal_distribution<double> dist_y(gps_y, std_y);\n",
    "  normal_distribution<double> dist_theta(theta, std_theta);\n",
    "\n",
    "  for (int i = 0; i < 3; ++i) {\n",
    "    double sample_x, sample_y, sample_theta;\n",
    "    \n",
    "    // TODO: Sample from these normal distributions like this: \n",
    "    //   sample_x = dist_x(gen);\n",
    "    //   where \"gen\" is the random engine initialized earlier.\n",
    "    sample_x = dist_x(gen);\n",
    "    sample_y = dist_y(gen);\n",
    "    sample_theta = dist_theta(gen);   \n",
    "     \n",
    "    // Print your samples to the terminal.\n",
    "    std::cout << \"Sample \" << i + 1 << \" \" << sample_x << \" \" << sample_y << \" \" \n",
    "              << sample_theta << std::endl;\n",
    "  }\n",
    "\n",
    "  return;\n",
    "}\n",
    "```\n",
    "### Prediction\n",
    "![](https://video.udacity-data.com/topher/2017/August/598a0d55_05-l-predictionstep.00-00-38-28.still001/05-l-predictionstep.00-00-38-28.still001.png)\n",
    "Now that we have initialized our particles it's time to predict the vehicle's position. Here we will use what we learned in the motion models lesson to predict where the vehicle will be at the next time step, by updating based on yaw rate and velocity, while accounting for Gaussian sensor noise.\n",
    "\n",
    "![](./img/26.png)\n",
    "\n",
    "### Update Step\n",
    "!(https://www.youtube.com/watch?v=1Uq2QZKz3aI)\n",
    "\n",
    "Note that the x and y errors are depicted from the point of view of the map (x is horizontal, y is vertical) rather than the point of view of the car where x is in the direction of the car’s heading,( i.e. It points to where the car is facing), and y is orthogonal (90 degrees) to the left of the x-axis (pointing out of the left side of the car).\n",
    "![](https://video.udacity-data.com/topher/2017/August/598a1664_07-l-data-association-nearest-neighbor.00-00-17-03.still003/07-l-data-association-nearest-neighbor.00-00-17-03.still003.png)\n",
    "![](https://video.udacity-data.com/topher/2017/August/598a167a_09-l-update-step.00-00-17-03.still001/09-l-update-step.00-00-17-03.still001.png)\n",
    "Now that we have incorporated velocity and yaw rate measurement inputs into our filter, we must update particle weights based on LIDAR and RADAR readings of landmarks. We will practice calculating particle weights, later in this lesson, with the Particle Weights Quiz.\n",
    "\n",
    "### Error\n",
    "![](./img/27.png)\n",
    "\n",
    "### Transformations and Associations\n",
    "In the project you will need to correctly perform observation measurement transformations, along with identifying measurement landmark associations in order to correctly calculate each particle's weight. Remember, our ultimate goal is to find a weight parameter for each particle that represents how well that particle fits to being in the same location as the actual car.\n",
    "\n",
    "In the quizzes that follow we will be given a single particle with its position and heading along with the car's observation measurements. We will first need to transform the car's measurements from its local car coordinate system to the map's coordinate system. Next, each measurement will need to be associated with a landmark identifier, for this part we will take the closest landmark to each transformed observation. Finally, we will use this information to calculate the weight value of the particle.\n",
    "![](https://video.udacity-data.com/topher/2017/August/598b467e_localization-map-concept-copy/localization-map-concept-copy.png)\n",
    "In the graph above we have a car (ground truth position) that observes three nearby landmarks, each one labeled OBS1, OBS2, OBS3. Each observation measurement has x, and y values in the car's coordinate system. We have a particle \"P\" (estimated position of the car) above with position (4,5) on the map with heading -90 degrees. The first task is to transform each observation marker from the vehicle's coordinates to the map's coordinates, with respect to our particle.\n",
    "\n",
    "### Converting Landmark Observations\n",
    "\n",
    "![](./img/28.png)\n",
    "Here is another example that might help your intuition.\n",
    "\n",
    "Referring to the figures below:\n",
    "\n",
    "Suppose the map coordinate system (grey lines) and the vehicle coordinate system (orange lines) are offset, as depicted below. If we know the location of the observation in vehicle coordinates (grey lines), we would need to rotate the entire system, observation included, -45 degrees to find it in map coordinates (grey lines), Once this rotation is done, we can easily see the location of the observation in map coordinates.\n",
    "#### Particle (blue dot) in Map Frame (grey)\n",
    "![](https://video.udacity-data.com/topher/2017/October/59d7d8a1_45deg-1/45deg-1.png)\n",
    "#### Particle (blue dot) in Vehicle Frame (orange)\n",
    "![](https://video.udacity-data.com/topher/2017/October/59d7d8d9_45deg-2/45deg-2.png)\n",
    "\n",
    "### Resources, Hints, and Tips\n",
    "Without implementation of localization methods the car does not know where it is within an acceptable level of precision. The car knows particle coordinates and observation coordinates. The objective is to use the particle coordinates and heading to transform the car's frame of reference to the map's frame of reference, associate the observations, then use the associated observations in the map domain to update the particle weight.\n",
    "Since we know the coordinates of the particle from the car's frame of reference we can use this information and a matrix rotation/translation to transform each observation from the car frame of reference to the map frame of reference. The particle is at (4,5) in the map coordinate system with a heading of -90 degrees. The figure indicates the heading by depicting the particle x-axis as pointing down (blue arrow). This is critical to understanding the matrix transformation we are about to perform.\n",
    "By convention we define the car coordinate system with x pointing up and y rotated from x by pi/2 (+90 degrees). This is another way of saying that y is perpendicular and to the left of x. When x is pointing down, we have the mirror of this, with y perpendicular to the right. To visualize this make an L with your left index finger and thumb with palm facing away from you, this is our map frame of reference. Point the thumb towards the ceiling, this is the car coordinate convention, now point your thumb down, this is the orientation of the particle at (4,5).\n",
    "Now consider the map frame of reference (make an L with your left index finger and thumb as above), this is a typical presentation of Cartesian coordinates, with x pointing right and y perpendicular to the left, pointing up. If we rotate our thumb down we have the particle orientation. To get back to the map orientation we must rotate counterclockwise by 90 degrees (thumb from pointing down back to pointing right). Try this a few times with your left hand. Notice that particle to map is a counterclockwise rotation (+90 degrees) nd map to particle is a clockwise rotation (-90 degrees).\n",
    "The most straight forward way of rotating and translating coordinates is through homogenous transformation matrix (see below) using the angle of rotation required to get to the particle’s frame from the map’s point of view, -90 degrees. This way we can use theta directly. The alternative, which we will not cover here is to is to use -theta and a transformation matrix from the particle frame to the map frame.\n",
    "This video is a great resource for developing a deeper understanding of how to solve this transformation problem - it covers the rotation transformation, and from there you just need to perform a translation.\n",
    "Observations in the car coordinate system can be transformed into map coordinates (\\text{x}_mx \n",
    "m\n",
    "​\t  and \\text{y}_my \n",
    "m\n",
    "​\t ) by passing car observation coordinates (\\text{x}_cx \n",
    "c\n",
    "​\t  and \\text{y}_cy \n",
    "c\n",
    "​\t ), map particle coordinates (\\text{x}_px \n",
    "p\n",
    "​\t  and \\text{y}_py \n",
    "p\n",
    "​\t ), and our rotation angle (-90 degrees) through a homogenous transformation matrix. This homogenous transformation matrix, shown below, performs rotation and translation.\n",
    "\n",
    "#### Associations\n",
    "Now that observations have been transformed into the map's coordinate space, the next step is to associate each transformed observation with a land mark identifier. In the map exercise above we have 5 total landmarks each identified as L1, L2, L3, L4, L5, and each with a known map location. We need to associate each transformed observation TOBS1, TOBS2, TOBS3 with one of these 5 identifiers. To do this we must associate the closest landmark to each transformed observation.\n",
    "\n",
    "As a reminder:\n",
    "\n",
    "TOBS1 = (6,3), TOBS2 = (2,2) and TOBS3 = (0,5).\n",
    "\n",
    "```cpp\n",
    "\n",
    "#include <cmath>\n",
    "#include <iostream>\n",
    "\n",
    "int main() {\n",
    "  // define coordinates and theta\n",
    "  double x_part, y_part, x_obs, y_obs, theta;\n",
    "  x_part = 4;\n",
    "  y_part = 5;\n",
    "  x_obs = 2;\n",
    "  y_obs = 2;\n",
    "  theta = -M_PI/2; // -90 degrees\n",
    "\n",
    "  // transform to map x coordinate\n",
    "  double x_map;\n",
    "  x_map = x_part + (cos(theta) * x_obs) - (sin(theta) * y_obs);\n",
    "\n",
    "  // transform to map y coordinate\n",
    "  double y_map;\n",
    "  y_map = y_part + (sin(theta) * x_obs) + (cos(theta) * y_obs);\n",
    "\n",
    "  // (6,3)\n",
    "  std::cout << int(round(x_map)) << \", \" << int(round((y_map)) << std::endl;\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "![](./img/29.png)\n",
    "\n",
    "                                                \n",
    "```cpp\n",
    "                                                #include <iostream>\n",
    "#include \"multiv_gauss.h\"\n",
    "\n",
    "int main() {\n",
    "  // define inputs\n",
    "  double sig_x, sig_y, x_obs, y_obs, mu_x, mu_y;\n",
    "  // define outputs for observations\n",
    "  double weight1, weight2, weight3;\n",
    "  // final weight\n",
    "  double final_weight;\n",
    "    \n",
    "  // OBS1 values\n",
    "  sig_x = 0.3;\n",
    "  sig_y = 0.3;\n",
    "  x_obs = 6;\n",
    "  y_obs = 3;\n",
    "  mu_x = 5;\n",
    "  mu_y = 3;\n",
    "  // Calculate OBS1 weight\n",
    "  weight1 = multiv_prob(sig_x, sig_y, x_obs, y_obs, mu_x, mu_y);\n",
    "  // should be around 0.00683644777551 rounding to 6.84E-3\n",
    "  std::cout << \"Weight1: \" << weight1 << std::endl;\n",
    "    \n",
    "  // OBS2 values\n",
    "  sig_x = 0.3;\n",
    "  sig_y = 0.3;\n",
    "  x_obs = 2;\n",
    "  y_obs = 2;\n",
    "  mu_x = 2;\n",
    "  mu_y = 1;\n",
    "  // Calculate OBS2 weight\n",
    "  weight2 = multiv_prob(sig_x, sig_y, x_obs, y_obs, mu_x, mu_y);\n",
    "  // should be around 0.00683644777551 rounding to 6.84E-3\n",
    "  std::cout << \"Weight2: \" << weight2 << std::endl;\n",
    "    \n",
    "  // OBS3 values\n",
    "  sig_x = 0.3;\n",
    "  sig_y = 0.3;\n",
    "  x_obs = 0;\n",
    "  y_obs = 5;\n",
    "  mu_x = 2;\n",
    "  mu_y = 1;\n",
    "  // Calculate OBS3 weight\n",
    "  weight3 = multiv_prob(sig_x, sig_y, x_obs, y_obs, mu_x, mu_y);\n",
    "  // should be around 9.83184874151e-49 rounding to 9.83E-49\n",
    "  std::cout << \"Weight3: \" << weight3 << std::endl;\n",
    "    \n",
    "  // Output final weight\n",
    "  final_weight = weight1 * weight2 * weight3;\n",
    "  // 4.60E-53\n",
    "  std::cout << \"Final weight: \" << final_weight << std::endl;\n",
    "    \n",
    "  return 0;\n",
    "}\n",
    "```\n",
    "                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
